{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this homework will use an HMM work to solve some  substitution ciphers. \n",
    "\n",
    "You are given some short encrypted texts from various English and Russian authors. Each was encoded using a different character substitution cipher. You need to identify the author of each. Plaintext data is provided with examples of each of the authors.\n",
    "\n",
    "\n",
    "This homework is worth **15 points** and is due by **2st Dec. 03:00**, please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: \n",
    "\n",
    "```\n",
    "filename\\tauthor\\tfirst_3_lines_of_decoded_text\n",
    "```\n",
    "    \n",
    "where 'filename' is the encrypted file from \"encrypted/\" and the name of the 'author' from \"plaintext/\" \n",
    "\n",
    "Using an HMM you will not be able to decode the text perfectly, but you'll see that it's often readable and you should be able to automatically identify the author by matching it against lines in the plaintext files.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T07:45:31.313736Z",
     "start_time": "2023-03-30T07:45:31.285006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in sorted(char_to_int_mapping.items(), key=lambda x: x[1])]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "\n",
    "    for sequence in sequences:\n",
    "        # Collect counts for initial state distribution (pi) and transition matrix (A)\n",
    "        pi_counts[sequence[0]] += 1\n",
    "        for i in range(1, len(sequence)):\n",
    "            A_counts[sequence[i-1], sequence[i]] += 1\n",
    "    \n",
    "    # Normalize counts to obtain parameter estimates\n",
    "    pi = pi_counts / np.sum(pi_counts)\n",
    "    A = A_counts / np.sum(A_counts, axis=1, keepdims=True)\n",
    "\n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T07:50:36.050558Z",
     "start_time": "2023-03-30T07:50:26.297667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = './encrypted/encrypted-10.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/encrypted-14.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T07:50:38.549154Z",
     "start_time": "2023-03-30T07:50:38.491849Z"
    }
   },
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping)\n",
    "        self.num_outputs = len(observations_to_char_mapping)\n",
    "        self.states_to_char_mapping = states_to_char_mapping\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping\n",
    "       \n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)\n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states)\n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs)\n",
    "        self.B /= np.sum(self.B, 1, keepdims=True) \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        if self.fixed_pi:\n",
    "            self.pi = parameters['pi']\n",
    "        self.fixed_A = 'A' in parameters\n",
    "        if self.fixed_A:\n",
    "            self.A = parameters['A']\n",
    "        self.fixed_B = 'B' in parameters\n",
    "        if self.fixed_B:\n",
    "            self.B = parameters['B']\n",
    "    \n",
    "        previous_llh = None\n",
    "        for iter in range(max_iters):\n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences)\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts)\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            print('iteration %d; log likelihood %.4f' % (iter, log_likelihood))\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            # Run Forward-Backward dynamic program\n",
    "            alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sequence)\n",
    "  \n",
    "            # Accumulate statistics.\n",
    "            pi_counts += gamma[:, 0]\n",
    "            A_counts += xi\n",
    "            for t, x in enumerate(sequence):\n",
    "                B_counts[:, x] += gamma[:, t]\n",
    "            \n",
    "            total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence)\n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        gamma = (alpha * beta) / np.sum(alpha * beta, 0)\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = np.zeros_like(self.A)\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    this_xi[i, j] += alpha[i, t] * self.A[i, j] * beta[j, t+1] * self.B[j, sequence[t+1]]        \n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        T = len(obs)\n",
    "        alpha = np.zeros((self.A.shape[0], T))\n",
    "\n",
    "        # base case\n",
    "        alpha[:, 0] = self.pi * self.B[:, obs[0]]\n",
    "\n",
    "        # recursive case\n",
    "        for t in range(1, T):\n",
    "            alpha[:, t] = np.dot(alpha[:, t-1], self.A) * self.B[:, obs[t]]\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def backward(self, obs):\n",
    "        T = len(obs)\n",
    "        beta = np.zeros((self.A.shape[0], T))\n",
    "\n",
    "        # base case\n",
    "        beta[:, T-1] = 1\n",
    "        \n",
    "        # recursive case\n",
    "        for t in range(T-2, -1, -1):\n",
    "            beta[:, t] = np.sum(beta[:, t+1] * self.A * self.B[:, obs[t+1]], 1)\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = [self.observations_to_char_mapping[x] for x in sequence]                \n",
    "            map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sequence)]\n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T07:52:38.440042Z",
     "start_time": "2023-03-30T07:50:40.916914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(states):       to    ote o  e o   e t  he    o e   o  o   oo  h\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       th    e  oe o   e te o      te      e      e \n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thee t t    tee  o   n e  e t  e e te    oe\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       to   e   e   o  ee   hee te  oe oe  o n  the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       w e  he  o te ee  oe e o e    o t i        oe\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       we     h  o te t  ee o          e n t  e\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       ao e  oe   e  o   e  ote te   o  e  o  e\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       we e e   t e   t o te ee  o n      te to o \n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       th  e   o    e  o     o  e o  o o  e  e\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te  o e e he     e  e he  o  e o   he   t i \n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 0; log likelihood -13974.4943\n",
      "(states):       to    ote o  e o   e t  hee   o e   o  a   oo th\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       th  e e  oe o   e te o  he  he      e te   e \n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thee t t    the  o   n e  e t  e e te    he\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       to  he   e   o  he   hee te  oe oe  h ne the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       whe  he  o te he  oe e o e    o t o        he\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       we  t  h  o th t  he o   e      e n t  e\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       to e  oe tee  o   e  ote te  eo  e  o  e\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       we e he  t e   t o he he  o t t  e th to o \n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       th  e t o    e  one   t  e o to o  e  e\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te  o e e he     e  e he  o  e o   he  nt o \n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 1; log likelihood -11961.1367\n",
      "(states):       to    ooe o  e o   e th hee e o e   o  t   to th\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       th  e e  oe o   e he hethe  he   ee e te  ee \n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thee t t    the  o   nee  e t  e e te h  he\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       th  he   er to the   hee th  oe he th ne the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       w e  he  o te he  oe e o t    o t o  s th  he\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       wer t  h  o th t  he o   e e    e t t  h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       to e  oe tee  i   e  ooe te eeo  e  o  h\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       we e he  t e   o o he he  o bet  e th no o \n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       th  e t o    e  one s t  e o to o se  e\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te to e e he     h  e he  o te o   he  no o \n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 2; log likelihood -11924.3484\n",
      "(states):       to   tooe o  e o  ne thenee e o e   o  b   th th\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       the e e toe o   e he hethe  he   ee e tentee \n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thee t t  t ine to   nee  e t ie e heth  he\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       the hes wer th ther thee th toe he thine the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       w e  he to te he  oe t t t i  o o o  s th  he\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       ber te h no th t ine o  ne e    e tet  h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       toee the the  i   e tooe heteeo  e  n  h\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       we e hen t n   i h he he  h bet  e th no o \n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       the e t e  t t tone s t ne d to o se  h\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te to e e hen  s h  e he  h te o   her no o \n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 3; log likelihood -11876.3959\n",
      "(states):       to t tooe o re o ine theneere h sen on b t th th\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       the e e the o   e ie hethes het  eete tentee \n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thee t te t ine to t nee  e t ie e iethe he\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       the hes ber th ther ehee th the he thine the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       ined he to te hentoe t t t in o i o  s the he\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       ber te h to te thine o  ne end  t tete h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       thee the thet in  t tooe ieteeo  ed n  h\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       we e hen tend hith ie hen h bet  e th wono \n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       the e tie  t t tone s t ne d wind se  h\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te to ere hend s h  t henth te n  ther noro \n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 4; log likelihood -11809.1374\n",
      "(states):       th t thon oare n ine thereeteth sendon bat th th\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       the e e the n   e ie hethes iet neete tenteet\n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thee h te t ine th t nee ie t ie e iethe he\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       the her ber thether enee th the he thine the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       aned ie th ie hentoe t wit it h inor s the he\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       ber te h th te thine n  ne and at tete h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       thee the thet in at thon ieteeo ied n  h\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       wene hen tend hith ie henth bet  e th wono \n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       the e winret t tone s binend winh set h\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te toiere hend s h at henth te n  ther noro \n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 5; log likelihood -11716.3689\n",
      "(states):       thit thon hare n ine therenteth nendot but th th\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       the eee the n  oe ie hethet iet neete tentent\n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       thes o te t ive thit mes ie t iete inthe he\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       the her bed thether aves bh the he thine the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       anet ie theie henthe t wit it heinot sethe he\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       bed we h theie thine n ine ant at mene h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       thes the tent it at thon ieteno int n  h\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(states):       wene hen tend hith ie henth betene th wonon\n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       the e windes t tone s bineth with set h\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te thinde hend s h at henth te n  ther noron\n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 6; log likelihood -11594.3477\n",
      "(states):       thit thin hare nline therenteth nondot but thech\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       whenean the nt oe in wethet int nente tentent\n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       when o te teive thit men ar thinte inthe re\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       the her ind thether aves by the we thine the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       annt in theid henthe t wit it heinot sethe re\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       and we h theid thave nline ant at mene h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       then the tenteit as thin intonotint nt h\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       weny hen ment hith in henth besene mh wonot\n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       whene wintes t tone s biteth with set h\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te thinde hend s y as henth te o ttoer norot\n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 7; log likelihood -11439.4425\n",
      "(states):       thit thin hare llire therenteth nondot but thaco\n",
      "(observations): zcazdzcoqdcuwhdqzawhdxihqhgzhzcdgeuwczdbuzdqcetq\n",
      "(states):       whenean the ntiny in wethet int rente tenment\n",
      "(observations): tchihegdzchdqzaiqdogdqhfihzdogvjuhgfhdfemmhgz\n",
      "(states):       when o te teive thit men ar thinty inthe re\n",
      "(observations): tchgdodxhifho hdzcazdmhgdaqdxjagzqdogfihaqh\n",
      "(states):       the hes ind thether aven by the we thane the\n",
      "(observations): fchhihkdagkdfchfnhkdh hgdbsdzchdqhjvqamhdqns\n",
      "(states):       aunt in theid honthe t wit it heinot setheare\n",
      "(observations): augzdogdzchoidseuzcvujdqaxdazdchowczdkhfihaqh\n",
      "(states):       and we d theid thave ltite ant as meme h\n",
      "(observations): agkdthaidzchoidbia hdqzazhdeuzdevdmhmeis\n",
      "(states):       then the tonteit as thin intonotint nt y\n",
      "(observations): zchgdzchdfegfhozdevdzcoqdogfegqzagzdqzas\n",
      "(states):       wety hon ment hith in honth besone my winot\n",
      "(observations): qhzqdseudmeqzdiofcdogdseuzcdbhveihdmsdqowcz\n",
      "(states):       whene wintes t tone s bateth with set y\n",
      "(observations): tchihdtaqzhvujdzomhdkhbazhzcdtozcdkhfas\n",
      "(states):       te thinde hond s y as honth te outtoer norot\n",
      "(observations): zedfcagwhdseuidkasdevdseuzcdzedqujjohkdgowcz\n",
      "iteration 8; log likelihood -11245.5928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26468\\350367188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Estimate the parameters and decode the encrypted sequences.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_with_em\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencrypted_sequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'pi'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26468\\2905920362.py\u001b[0m in \u001b[0;36mestimate_with_em\u001b[1;34m(self, sequences, parameters, epsilon, max_iters)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Infer expected counts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mpi_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Update parameters based on counts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26468\\2905920362.py\u001b[0m in \u001b[0;36me_step\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Run Forward-Backward dynamic program\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Accumulate statistics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26468\\2905920362.py\u001b[0m in \u001b[0;36mforward_backward\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                     \u001b[0mthis_xi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mxi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mthis_xi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_xi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm.estimate_with_em(encrypted_sequences[:100], parameters={'pi': pi, 'A': A})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/encrypted-5.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text by decoding some of it and matching it against the samples provided for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

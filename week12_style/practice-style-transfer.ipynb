{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Week 10 - text style transfer\n\nHello, sitzen class A.412C!\n\nBased on your browser search history, we conclude that you have an above average skill in natural language processing. In our benevolence, we give you a chance to contribute your skills to upholding the happiest society in the universe. Are you up to the task?\n\nAs you know, our most recent breakthrough was replacing 97% restaurant workers with BFGHQBERT+++ autonomous food dispensers.\n\nYet a some radical elements failed to recognize the greater good that we brought them. They mistakenly voice their ignorant opinions about our new INGSOC-approved restaurants, brining dangerous doubt to the minds of our loyal citzens.\n\nSurely you cannot tolerate such infidelity! Our loyal citzens demand that you rectify their mistake. _You must build a model that will automatically improve their ignorant thoughts and replace them with the thoughts they should actually have._\n\nAttached below are the INGSOC-approved datasets for ignorant and correct thoughts. The scientific terminology is for wrong opinions and correct opinions is \"negative\" and \"positive\", respectively.\n\nRespond within 7 days or you will lose 3.7629 citzenship points.\n\n![img](https://ih1.redbubble.net/image.1254830934.9884/poster,504x498,f8f8f8-pad,400x240,f8f8f8.jpg)","metadata":{"id":"hYvnVzA2DmIu"}},{"cell_type":"code","source":"!pip install -q transformers\n!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.train.0 -O train_negative\n!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.train.1 -O train_positive\n!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.dev.0 -O dev_negative\n!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.dev.1 -O dev_positive","metadata":{"id":"Z63QypDjVmIe","outputId":"98c5d80e-03ed-48bd-ab57-ffc285924ba5","execution":{"iopub.status.busy":"2023-04-01T16:19:27.680400Z","iopub.execute_input":"2023-04-01T16:19:27.681134Z","iopub.status.idle":"2023-04-01T16:19:45.118895Z","shell.execute_reply.started":"2023-04-01T16:19:27.681094Z","shell.execute_reply":"2023-04-01T16:19:45.117534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head -n 5 ./dev_positive\n!echo\n!head -n 5 ./dev_negative","metadata":{"id":"oQADoMFcU5cU","outputId":"65ca9d5a-b9f8-44f3-96b8-bab17e010212","execution":{"iopub.status.busy":"2023-04-01T16:19:45.122327Z","iopub.execute_input":"2023-04-01T16:19:45.123707Z","iopub.status.idle":"2023-04-01T16:19:48.070140Z","shell.execute_reply.started":"2023-04-01T16:19:45.123658Z","shell.execute_reply":"2023-04-01T16:19:48.068944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nif device == 'cpu':\n    print(\"Fine-tuning BERT without an accelerator is not party-approved.\")","metadata":{"id":"2MTzCt4i6BE-","outputId":"1c4bbdf7-d084-46f6-a91a-d9a66b48b7d6","execution":{"iopub.status.busy":"2023-04-01T16:19:48.071847Z","iopub.execute_input":"2023-04-01T16:19:48.073018Z","iopub.status.idle":"2023-04-01T16:19:48.139152Z","shell.execute_reply.started":"2023-04-01T16:19:48.072974Z","shell.execute_reply":"2023-04-01T16:19:48.137964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 1: Masked language model\n\nAttached below you can find the INGSOC-compliant training code that fine-tunes a BERT model for Masked Language Modeling.\n\nYou shall use this model to generate positive replacements for negative tokens.","metadata":{"id":"VGhzg7qKKdqX"}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForMaskedLM\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_mlm_positive = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True).to(device).train(True)","metadata":{"id":"VhhZG7YMVihR","outputId":"0be547c8-7862-48ad-89a8-37a4e236d297","execution":{"iopub.status.busy":"2023-04-01T16:19:48.143551Z","iopub.execute_input":"2023-04-01T16:19:48.143865Z","iopub.status.idle":"2023-04-01T16:20:00.293385Z","shell.execute_reply.started":"2023-04-01T16:19:48.143837Z","shell.execute_reply":"2023-04-01T16:20:00.292350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\n\nprint(\"Preparing the training data...\")\ntrain_positive_dataset = LineByLineTextDataset(\n    file_path=\"./train_positive\", tokenizer=tokenizer, block_size=128)\n\ndev_positive_dataset = LineByLineTextDataset(\n    file_path=\"./dev_positive\", tokenizer=tokenizer, block_size=128)\n\nprint(\"Dataset ready!\")\n\ntrainer = Trainer(\n    model=bert_mlm_positive,\n    train_dataset=train_positive_dataset, eval_dataset=dev_positive_dataset,\n    data_collator=DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=True, mlm_probability=0.15),\n    args=TrainingArguments(\n        output_dir=\"./bert_mlm_positive\", overwrite_output_dir=True,\n        num_train_epochs=10, per_device_train_batch_size=32,\n        save_steps=10_000, save_total_limit=2,\n        evaluation_strategy='epoch'),\n)\n\ntrainer.train()","metadata":{"id":"A5Ir_RGWBWMF","outputId":"be2a4c1c-331a-44c0-e561-12977a0663a4","execution":{"iopub.status.busy":"2023-04-01T16:22:29.516115Z","iopub.execute_input":"2023-04-01T16:22:29.516549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_mlm_negative = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True).to(device).train(True)\n\nprint(\"Preparing the training data...\")\ntrain_negative_dataset = LineByLineTextDataset(\n    file_path=\"./train_negative\", tokenizer=tokenizer, block_size=128)\n\ndev_negative_dataset = LineByLineTextDataset(\n    file_path=\"./dev_negative\", tokenizer=tokenizer, block_size=128)\n\ntrainer = Trainer(\n    model=bert_mlm_negative,\n    train_dataset=train_negative_dataset, eval_dataset=dev_negative_dataset,\n    data_collator=DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=True, mlm_probability=0.15),\n    args=TrainingArguments(\n        output_dir=\"./bert_mlm_negative\", overwrite_output_dir=True,\n        num_train_epochs=10, per_device_train_batch_size=32,\n        save_steps=10_000, save_total_limit=2,\n        evaluation_strategy='epoch'),\n)\n\ntrainer.train()","metadata":{"id":"LO0jC8_7OWrq","outputId":"cb9400a3-6889-4600-fe1b-b8810f0cf3c9","execution":{"iopub.status.busy":"2023-04-01T17:54:55.584657Z","iopub.execute_input":"2023-04-01T17:54:55.585104Z","iopub.status.idle":"2023-04-01T18:50:36.817577Z","shell.execute_reply.started":"2023-04-01T17:54:55.585063Z","shell.execute_reply":"2023-04-01T18:50:36.816687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 2: Replace tokens\n\nYou can now use the two masked language models to align user opinions. You can do so with the following steps:\n\n1. Find tokens where the ratio $(P_{positive}(x) + \\epsilon) / (P_{negative}(x) + \\epsilon)$ is the smallest\n2. Replace those tokens with one of $k$ most likely tokens according to $P_{positive}(x)$.\n3. Rinse, repeat\n\nYou can find the full procedure at https://arxiv.org/abs/2010.01054","metadata":{"id":"x_IQohMhO63b"}},{"cell_type":"code","source":"def get_replacements(sentence: str, num_tokens=4, k_best=4, epsilon=1e-3):\n    \"\"\"\n    - split the sentence into tokens using the INGSOC-approved BERT tokenizer\n    - find :num_tokens: tokens with the highest ratio (see above)\n    - replace them with :k_best: words according to bert_mlm_positive\n    :return: a list of all possible strings (up to k_best * num_tokens)\n    \"\"\"\n    sentence_ix = tokenizer(sentence, return_tensors='pt')\n    sentence_ix = {key: value.to(device) for key, value in sentence_ix.items()}\n\n    probs_positive = bert_mlm_positive(**sentence_ix).logits.softmax(dim=-1)[0]\n    probs_negative = bert_mlm_negative(**sentence_ix).logits.softmax(dim=-1)[0]\n\n    p_tokens_positive = probs_positive[torch.arange(len(probs_positive)), sentence_ix['input_ids'][0]]\n    p_tokens_negative = probs_negative[torch.arange(len(probs_negative)), sentence_ix['input_ids'][0]]\n\n    idx_to_replace = ((p_tokens_positive + epsilon) / (p_tokens_negative + epsilon)).argsort()[:num_tokens]\n    replaced_variants = []\n    for idx in idx_to_replace:\n        for token_idx in probs_positive[idx].argsort()[-k_best:]:\n            sentence_ix_cur = sentence_ix.copy()\n            sentence_ix_cur['input_ids'][0][idx] = token_idx\n            replaced_variants.append(tokenizer.decode(sentence_ix_cur['input_ids'][0][1:-1]))\n\n    return replaced_variants","metadata":{"id":"bVfJC5_kRskk","execution":{"iopub.status.busy":"2023-04-01T18:51:02.702102Z","iopub.execute_input":"2023-04-01T18:51:02.702507Z","iopub.status.idle":"2023-04-01T18:51:02.715311Z","shell.execute_reply.started":"2023-04-01T18:51:02.702473Z","shell.execute_reply":"2023-04-01T18:51:02.713844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_replacements(\"great wings and decent drinks but the wait staff is horrible !\",\n                 num_tokens=2, k_best=2)\n# >>> [\"great wings and decent drinks but the wait staff is great !\", \"great wings and decent drinks but the wait staff is awesome !\"])","metadata":{"id":"zWtf75uZROwP","outputId":"0564ddce-5a40-41c9-f1fe-f19fb7e150b9","execution":{"iopub.status.busy":"2023-04-01T18:51:03.699554Z","iopub.execute_input":"2023-04-01T18:51:03.699928Z","iopub.status.idle":"2023-04-01T18:51:03.769614Z","shell.execute_reply.started":"2023-04-01T18:51:03.699895Z","shell.execute_reply":"2023-04-01T18:51:03.767697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 3: Classifier & beam search (5 pts)\n\nSometimes the roots of dissent too deep to rip out with single word replacements. If you truly are a class A412C citzen, surely you understand what it means.\n\nIn order to better serve your fellow citzens, you must improve your solution. Train a classifier model that will separate the negative (ignorant) opinions from positive ones.\n\nWith this classifier you can now generate multiple best hypotheses and search for the ones that have the highest $P_{classifier}(\\text{positive} | x)$.","metadata":{"id":"jx831HfdMRh4"}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nbert_sentiment = BertForSequenceClassification.from_pretrained('Ghost1/bert-base-uncased-finetuned_for_sentiment_analysis1-sst2').to(device).train(True)\n\ndef get_positive_log_prob(sentence):\n    input_ids = tokenizer(sentence, return_tensors='pt').to(device)\n    with torch.no_grad():\n        logits = bert_sentiment(**input_ids).logits\n    probs = torch.softmax(logits, dim=-1)\n    pos_prob = probs[:, 1].item()\n    return pos_prob\n\nget_positive_log_prob('i love this movie so much')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:22:22.890318Z","iopub.execute_input":"2023-04-01T19:22:22.891443Z","iopub.status.idle":"2023-04-01T19:22:24.436176Z","shell.execute_reply.started":"2023-04-01T19:22:22.891371Z","shell.execute_reply":"2023-04-01T19:22:24.434095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# your final task is to build a beam search-like procedure that iteratively\n# generates candidates using MLM and selects M best with classifier\n\n# as before, your fellow citzens request that you show your loyalty by \n# writing a short report on how your method works and demonstrating\n# the effectiveness of your system works with at least 10 examples\n\n# Note: as a class >=A410 citzen, you are entitled to creativity level 2.1:\n# you may modify the search objective by using language models, different search procedures\n# or implement a completely different style transfer method.\n\ndef beam_search_results(sentence, beam_size=4, positivity_threshold=0.7):\n    cur_beam_samples = [sentence]\n    # here we can add some other metrics, not only positivity \n    # also we can stop after n replacements\n    while get_positive_log_prob(cur_beam_samples[0]) < positivity_threshold:\n        all_replacements = []\n        for sample in cur_beam_samples:\n            for replacement in get_replacements(sample):\n                all_replacements.append(replacement)\n        replacements_with_scores = []\n        for replacement in all_replacements:\n            replacements_with_scores.append((get_positive_log_prob(replacement), replacement))\n        replacements_with_scores.sort(key=lambda x: x[0], reverse=True)\n        cur_beam_samples[:] = []\n        for replacement_num in range(beam_size):\n            cur_beam_samples.append(replacements_with_scores[replacement_num][1])\n    \n    return cur_beam_samples[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:09:49.398962Z","iopub.execute_input":"2023-04-01T19:09:49.399343Z","iopub.status.idle":"2023-04-01T19:09:49.414206Z","shell.execute_reply.started":"2023-04-01T19:09:49.399310Z","shell.execute_reply":"2023-04-01T19:09:49.412643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_data = list(open('./dev_negative'))\n\nfor sentence in dev_data[500:530]:\n    print(f'Unmodified sentence: {sentence}')\n    print(f'Sentence after styling: {beam_search_results(sentence)}')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:12:30.083574Z","iopub.execute_input":"2023-04-01T19:12:30.084701Z","iopub.status.idle":"2023-04-01T19:12:42.434258Z","shell.execute_reply.started":"2023-04-01T19:12:30.084654Z","shell.execute_reply":"2023-04-01T19:12:42.432962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 4 Deployment! (5 points)\n\nBy now you have built a model that can change the style of your reviews. But what of the others? There's circa 8 Billion of us and only one of you, so it is only right that you share your invention with everyone of us.\n\nYour final task is to __build a web interface around your model that others can use in their browser__.\n\nThere are many ways you can do so, one of them being TensorFlow.js we learned a few weeks ago. You can solve this task any way you want _provided that an INGSOC-certified teaching assistant will be able to view it in their browser_.\nBelow we cover one (arguably) simplest way using streamlit and pure python.\n\n[Streamlit](https://streamlit.io/) is a simple python-based framework for developing interactive ML apps that run python on the backend. You define your frontend using a combination of markdown and widgets such as text inputs, charts, checkboxes, etc.\n\nYou can install streamlit as `pip install streamlit`, but __please switch from colab to your local computer__, otherwise you won't be able to view the results your work!\n\nLet's walk through a simple streamlit app:\n\n```python\nimport streamlit as st\nst.set_page_config(page_title=\"A + B calculator pro max\", layout=\"centered\")\nst.markdown(\"## Hello, world!\")\na = st.number_input(\"A =\", value=0)\nb = st.number_input(\"B =\", value=0)\na_plus_b = a + b\nst.markdown(f\"$A + B = {a_plus_b}$\")\n```\n\nYou can start this app on your computer like this:\n```streamlit run app.py``` where app.py is your python file name.\n\n![img](https://i.imgur.com/GUTjZQC.png)\n\n\nYou can host this app in three ways: [streamlit cloud](https://streamlit.io/cloud), [huggingface spaces](https://huggingface.co/new-space) or on your own server. All of them are possible for free, but we recommend the middle one out as it is the simplest.\n\nYou can find the full tutorial on hosting with huggingface spaces here: https://huggingface.co/docs/hub/spaces","metadata":{"id":"9USIonXb43h8"}}]}